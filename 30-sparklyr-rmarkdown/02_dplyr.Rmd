---
title: "Manipulating Data with dplyr"
output: html_notebook
---

## Overview

[**dplyr**](https://cran.r-project.org/web/packages/dplyr/index.html) is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:

* Select, filter, and aggregate data
* Use window functions (e.g. for sampling)
* Perform joins on DataFrames
* Collect data from Spark into R

Statements in dplyr can be chained together using pipes defined by the [magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) R package. dplyr also supports [non-standard evalution](https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html) of its arguments. For more information on dplyr, see the [introduction](https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html), a guide for connecting to [databases](https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html), and a variety of [vignettes](https://cran.r-project.org/web/packages/dplyr/index.html). 


### Flights Data

This guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the `nycflights13` R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US [Bureau of Transportation Statistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0), and is documented in `?nycflights13`

Connect to the cluster and copy the flights data using the `copy_to` function. Caveat: The flight data in `nycflights13` is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects. 

```{r message=FALSE, warning=FALSE}
library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)
sc <- spark_connect(master = "local", version = "2.0.0")
flights <- copy_to(sc, flights, "flights")
airlines <- copy_to(sc, airlines, "airlines")
```

## dplyr Verbs

Verbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:

* `select` ~ `SELECT`
* `filter` ~ `WHERE`
* `arrange` ~ `ORDER`
* `mutate` ~ `operators: +, *, log, etc.`
* `summarise` ~ `aggregators: sum, min, sd, etc.`

```{r}
flights %>%
  select(month, day, carrier, air_time) %>%
  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%
  arrange(carrier) %>%
  mutate(air_time_hours = air_time / 60)
```

## Grouping

The `group_by` function corresponds to the `GROUP BY` statement in SQL.

```{r, eval=FALSE}
flights %>%
  group_by(month) %>%
  summarize(count = n(), mean_dep_delay = mean(dep_delay), sd_dep_delay = sd(dep_delay)) %>%
  arrange(month)
```


## Window Functions

dplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using `sql_render()`.

```{r}
# Rank the top three largest delays for each carrier
flights %>%
  group_by(carrier) %>%
  mutate(rank = rank(desc(dep_delay))) %>%
  filter(rank <= 3) %>%
  select(carrier, year, month, day, dep_delay, rank)
```

## Peforming Joins

It's rare that a data analysis involves only a single table of data. In practice, you'll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:

* Mutating joins, which add new variables to one table from matching rows in 
  another.

* Filtering joins, which filter observations from one table based on whether or 
  not they match an observation in the other table.

* Set operations, which combine the observations in the data sets as if they 
  were set elements.

All two-table verbs work similarly. The first two arguments are `x` and `y`, and provide the tables to combine. The output is always a new table with the same type as `x`.

The following statements are equivalent:

```{r}
flights %>% 
  left_join(airlines, by = "carrier") %>%
  select(name, carrier, flight, origin, dest, dep_delay) %>%
  filter(origin == "JFK", dest == "SFO") %>%
  arrange(desc(dep_delay))
```

## SQL Translation

It's relatively straightforward to translate R code to SQL (or indeed to any programming language) when doing simple mathematical operations of the form you normally use when filtering, mutating and summarizing. dplyr knows how to convert the following R functions to Spark SQL:

```r
# Basic math operators
+, -, *, /, %%, ^
  
# Math functions
abs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan, tanh

# Logical comparisons
<, <=, !=, >=, >, ==, %in%

# Boolean operations
&, &&, |, ||, !

# Character functions
paste, tolower, toupper, nchar

# Casting
as.double, as.integer, as.logical, as.character, as.date

# Basic aggregations
mean, sum, min, max, sd, var, cor, cov, n
```

```{r}
sql0 <- flights %>%
  select(carrier, dep_delay, origin) %>%
  mutate(target = ifelse(origin == "JFK", 1L, 0L)) %>% # case statement
  mutate(rank = rank(desc(dep_delay))) %>% # windows function
  left_join(airlines, by = "carrier") # join

sql_render(sql0)
```


## Laziness

When working with databases, dplyr tries to be as lazy as possible:

* It never pulls data into R unless you explicitly ask for it.

* It delays doing any work until the last possible moment: it collects together
  everything you want to do and then sends it to the database in one step.

For example, take the following code:

```{r}
sql1 <- flights %>%
  select(origin, dest, month, day, air_time) %>%
  filter(origin == "JFK", dest == "SFO", air_time > 0) %>%
  arrange(desc(air_time))

sql2 <- sql1 %>%
  mutate(air_time_hours = air_time / 60)

sql2
```

## Register and cache

You can register tables with the Hive metadata store.

```{r}
sdf_register(sql2, "jfk2sfo")
tbl_cache(sc, "jfk2sfo")
jfk2sfo_tbl <- tbl(sc, "jfk2sfo")
```

## Collecting to R

You can copy data from Spark into R's memory by using `collect()`. 

```{r}
jfk2sfo <- collect(jfk2sfo_tbl)
jfk2sfo
```

`collect()` executes the Spark query and returns the results to R for further analysis and visualization.

```{r}
# Test the significance of pairwise differences and plot the results
with(jfk2sfo, pairwise.t.test(air_time_hours, month))
ggplot(jfk2sfo, aes(month, air_time_hours, group=month)) + geom_boxplot()
```


## Sampling

You can use `sample_n()` and `sample_frac()` to take a random sample of rows: use `sample_n()` for a fixed number and `sample_frac()` for a fixed fraction.

```{r}
sample_n(flights, 10)
sample_frac(flights, 0.01)
```

## Writing Data

It is often useful to save the results of your analysis or the tables that you have generated on your Spark cluster into persistent storage. The best option in many scenarios is to write the table out to a [Parquet](https://parquet.apache.org/) file using the [spark_write_parquet](reference/sparklyr/spark_write_parquet.html) function. For example:

```{r}
spark_write_parquet(tbl, "hdfs://hdfs.company.org:9000/hdfs-path/data")
```

This will write the Spark DataFrame referenced by the tbl R variable to the given HDFS path. You can use the [spark_read_parquet](reference/sparklyr/spark_read_parquet.html) function to read the same table back into a subsequent Spark session:

```{r}
tbl <- spark_read_parquet(sc, "data", "hdfs://hdfs.company.org:9000/hdfs-path/data")
```

You can also write data as CSV or JSON using the [spark_write_csv](reference/sparklyr/spark_write_csv.html) and [spark_write_json](reference/sparklyr/spark_write_json.html) functions.
